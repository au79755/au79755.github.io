---
title: "Validating AI Model Generalization Across Languages"
collection: projects
date: 2023-05-01
excerpt: "An award-winning research project that investigated the critical business risk of data bias in AI, quantifying the performance gap of an English-trained model on unseen languages."
permalink: /projects/2023-ai-bias
---

This award-winning project tackled the multi-million-dollar risk for any AI company with global ambitions: product failure due to biased training data.

The core finding proved that a "one-size-fits-all" model, trained only on English, is not a viable strategy for international deployment. Performance across unseen languages was unpredictable, revealing significant hidden risks. This work produced a clear framework for de-risking global AI launches by validating the need for market-by-market testing and data localization, while reaffirming the need to thoroughly verify claims of multilingual capabilities across AI offerings.

### Recognition
* **Honorable Mention**, University of Illinois Undergraduate Research Symposium

### Tech & Skills
* **Core Competencies:** AI Bias & Fairness, Data-Driven Risk Analysis, Model Generalization, Go-to-Market Strategy

